{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7bbcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import community as community_louvain\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from colorama import Fore, Style\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ae7b0",
   "metadata": {},
   "source": [
    "# PROJECT 8: TripAdvisor European restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30740eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General parameters \n",
    "\n",
    "number_of_nodes = 1000 # Number of nodes to work with for the network (between 0 and 1000000, between 1000 and 10000 recommanded, depending of the computer)\n",
    "same_dataset = False # Activate it to use the same dataset for all analysis (recommanded to let it on True)\n",
    "random_state = 88 # Change to test an other subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d2c5b",
   "metadata": {},
   "source": [
    "#### 1: Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "\n",
    "columns_to_keep = [\n",
    "    'restaurant_name',\n",
    "    'restaurant_link',\n",
    "    'original_location',\n",
    "    'country',\n",
    "    'cuisines',\n",
    "    'special_diets',\n",
    "    'features',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'popularity_generic',\n",
    "    'total_reviews_count',\n",
    "    'avg_rating',\n",
    "    'keywords',\n",
    "    'vegetarian_friendly',\n",
    "    'vegan_options',\n",
    "    'gluten_free',\n",
    "    'awards'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\"tripadvisor_european_restaurants.csv\")[columns_to_keep]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc897db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put list of element into list\n",
    "\n",
    "def split_string_to_list(s):\n",
    "    if isinstance(s, str):\n",
    "        return [item.strip() for item in s.split(\",\")]\n",
    "    return []\n",
    "\n",
    "for column in [\"cuisines\", \"special_diets\", \"features\", \"keywords\"]:\n",
    "    df[column] = df[column].apply(split_string_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3913b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling data\n",
    "\n",
    "if same_dataset :\n",
    "    df_keywords = df[df['keywords'].apply(lambda x: isinstance(x, list) and len(x) > 0)].reset_index(drop=True)\n",
    "    df_sample = df_keywords.sample(n=number_of_nodes, random_state=random_state).reset_index(drop=True)\n",
    "else:\n",
    "    df_sample = df.sample(n=number_of_nodes, random_state=random_state).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d41ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8911d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the graph\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adding nodes\n",
    "for idx, row in df_sample.iterrows():\n",
    "    G.add_node(idx, name=row['restaurant_name'], city=row['original_location'].replace(\"[\", \"\").replace(\"]\", \"\").replace(' \"', \"\").replace('\"', \"\").split(\",\")[-1] # corresponding to the city\n",
    "               , cuisines=row['cuisines'], special_diets=row['special_diets'], features=row.get('features', []), popularity_generic=row['popularity_generic'], total_reviews_count=row['total_reviews_count'], \n",
    "               avg_rating=row['avg_rating'], keywords=row['keywords'], vegetarian_friendly=row['vegetarian_friendly'], vegan_options=row['vegan_options'], gluten_free=row['gluten_free'])\n",
    "\n",
    "index = defaultdict(set)\n",
    "\n",
    "for idx, row in df_sample.iterrows():\n",
    "    tags = row['cuisines'] + row['special_diets'] + row.get('features', []) + [row['original_location'].replace(\"[\", \"\").replace(\"]\", \"\").replace('\"', \"\").replace(\" \", \"\").split(\",\")[-1]]\n",
    "    for tag in tags:\n",
    "        index[tag].add(idx)\n",
    "\n",
    "# Adding edges\n",
    "added_edges = set()\n",
    "\n",
    "for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Building graph\"):\n",
    "    tags = row['cuisines'] + row['special_diets'] + row.get('features', []) + [row['original_location'].replace(\"[\", \"\").replace(\"]\", \"\").replace('\"', \"\").replace(\" \", \"\").split(\",\")[-1]]\n",
    "    neighbors = set()\n",
    "    for tag in tags:\n",
    "        neighbors.update(index[tag])\n",
    "    neighbors.discard(idx)\n",
    "\n",
    "    for neighbor in neighbors:\n",
    "        if (neighbor, idx) in added_edges or (idx, neighbor) in added_edges:\n",
    "            continue\n",
    "\n",
    "        r2 = df_sample.loc[neighbor]\n",
    "\n",
    "        shared = len(set(row['cuisines']) & set(r2['cuisines']))\n",
    "        if shared == 0:\n",
    "            shared += len(set(row['special_diets']) & set(r2['special_diets']))\n",
    "            if shared == 0:    \n",
    "                shared += len(set(row.get('features', [])) & set(r2.get('features', [])))\n",
    "                if shared == 0:\n",
    "                    shared += 1 if row['original_location'].replace(\"[\", \"\").replace(\"]\", \"\").replace('\"', \"\").replace(\" \", \"\").split(\",\")[-1] == r2['original_location'].replace(\"[\", \"\").replace(\"]\", \"\").replace('\"', \"\").replace(\" \", \"\").split(\",\")[-1] else 0\n",
    "        if shared > 0:\n",
    "            G.add_edge(idx, neighbor)\n",
    "            added_edges.add((idx, neighbor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a4e3b2",
   "metadata": {},
   "source": [
    "#### 2: Weighted Network Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b84530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the haversine distance based on lattitude and longitude\"\"\"\n",
    "    R = 6371\n",
    "    phi1, phi2 = math.radians(lat1), math.radians(lat2)\n",
    "    delta_phi = math.radians(lat2 - lat1)\n",
    "    delta_lambda = math.radians(lon2 - lon1)\n",
    "    \n",
    "    # passing from circular coordinates to carthesian coordinates \n",
    "    a = math.sin(delta_phi / 2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def add_weighted_edges(G, df_sample):\n",
    "    \"\"\"Adding weighted edge to the graph based on a similarity score. A similarity score maximum :\n",
    "        - two restaurant at the same place and with exactly same features will gave a weight of 0\n",
    "        - two restaurants separated by a distance above 5km and without any common features will gave a weight of 2\n",
    "        - common cuisines, special_diets and features are adding to the weight a value between 0 and 1\n",
    "        - distance add to weight a value between 0 (distance of 0) and 1 (distance over 5km)\"\"\"\n",
    "    for idx1, row1 in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Building graph\"):\n",
    "        for idx2, row2 in df_sample.iterrows():\n",
    "            if idx1 >= idx2:\n",
    "                continue\n",
    "            \n",
    "            # Attribute a similarity score base on shared tags from two restaurants and geographic proximity\n",
    "\n",
    "            common_cuisines = set(row1['cuisines']).intersection(row2['cuisines']) # count number of \"cuisines\" common tags\n",
    "            common_diets = set(row1['special_diets']).intersection(row2['special_diets'])  # count number of \"special_diets\" common tags\n",
    "            common_features = set(row1['features']).intersection(row2['features'])  # count number of \"features\" common tags\n",
    "\n",
    "            similarity_score = len(common_cuisines) + len(common_diets) + len(common_features) # calculate a prliminary score\n",
    "\n",
    "            max_similarities = max(len(row1['cuisines']), len(row2['cuisines'])) + max(len(row1['special_diets']), len(row2['special_diets'])) + max(len(row1['features']), len(row2['features']))\n",
    "\n",
    "            if max_similarities == 0:\n",
    "                weight = 1 # the case of there are no similarities possible because some informations are missing\n",
    "            else:\n",
    "                weight = (max_similarities - similarity_score) / max_similarities\n",
    "\n",
    "            # increase the similarity score if restaurants are close (less than 5km between both two)\n",
    "\n",
    "            distance = haversine(row1['latitude'], row1['longitude'], row2['latitude'], row2['longitude'])\n",
    "            \n",
    "            if G.has_edge(idx1, idx2):\n",
    "                distance = 4.99 # if restaurant are in the same city but we can't calculate the haversine distance or if the haversine distance is over 5km, set the distance to the maximum sub 5\n",
    "            \n",
    "            elif str(distance) == \"nan\":\n",
    "                distance = 5 # to avoid problems with restaurants unlocated\n",
    "\n",
    "            if distance < 5:\n",
    "                weight = weight +  (distance / 5) # if restaurant are close, attribute score is increased with a ponderation of this distance\n",
    "            else:\n",
    "                weight = weight + 1\n",
    "            \n",
    "            if weight == 0:\n",
    "                weight = 0.0001 # to avoid problems with a weight of 0\n",
    "\n",
    "            if weight < 2:\n",
    "                G.add_edge(idx1, idx2, weight=weight, distance=distance) # for the recommandation algorithm is better to have the distance\n",
    "    \n",
    "    return G\n",
    "\n",
    "G = add_weighted_edges(G, df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af5019",
   "metadata": {},
   "source": [
    "#### 3: Degree and Strength Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e326155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the degree (number of edge)\n",
    "degree = dict(G.degree())\n",
    "\n",
    "# Calculate the ponderate degree (sum of weigth of edges)\n",
    "weighted_degree = dict(G.degree(weight='weight'))\n",
    "\n",
    "# Plot the 10 most connected restaurants based on the degree\n",
    "print(\"Top 10 most connected restaurants based on the degree :\")\n",
    "top_degree = sorted(degree.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for id, deg in top_degree:\n",
    "    print(f\"{G.nodes[id][\"name\"]}: {deg} \")\n",
    "\n",
    "# Plot the 10 most connected restaurants based on the ponderated degree\n",
    "print(\"\\nTop 10 most connected restaurants based on the pondered degree :\")\n",
    "top_weighted_degree = sorted(weighted_degree.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for id, weighted_deg in top_weighted_degree:\n",
    "    print(f\"{G.nodes[id][\"name\"]}: {weighted_deg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a7e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time, the ranking is based on the best ration weight / nb_of_edges\n",
    "\n",
    "# Calculating ratio only for nodes with more than n edges\n",
    "\n",
    "n_egdes = 100\n",
    "\n",
    "average_weight_ratio = {}\n",
    "for node in G.nodes():\n",
    "    deg = degree.get(node, 0)\n",
    "    weighted_deg = weighted_degree.get(node, 0)\n",
    "    if deg > n_egdes:\n",
    "        average_weight_ratio[node] = weighted_deg / deg\n",
    "\n",
    "print(\"\\nTop 10 most connected restaurants based on the best ratio total_weight / nb_of_edge :\")\n",
    "lowest_ratio = sorted(average_weight_ratio.items(), key=lambda x: x[1])[:10]\n",
    "for id, ratio in lowest_ratio:\n",
    "    deg = degree.get(id, 0)\n",
    "    print(f\"{id}, {G.nodes[id]['name']}: {ratio:.4f} ; {G.nodes[id]['city']} ; deg={deg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8da508",
   "metadata": {},
   "source": [
    "#### 4: Centrality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe52333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "print(\"degree_centrality done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2651ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "closeness_centrality = nx.closeness_centrality(G, distance='weight')\n",
    "print(\"closeness_centrality done\")\n",
    "betweenness_centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "print(\"betweenness_centrality done\")\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G, weight='weight', max_iter=1000)\n",
    "print(\"eigenvector_centrality done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to sorted lists\n",
    "def top_n(dic, n=10):\n",
    "    return sorted(dic.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "print(\"Top 10 restaurants by degree centrality:\")\n",
    "for node, score in top_n(degree_centrality):\n",
    "    print(f\"{G.nodes[node]['name']}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd1604",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 10 restaurants by closeness centrality:\")\n",
    "for node, score in top_n(closeness_centrality):\n",
    "    print(f\"{G.nodes[node]['name']}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e304e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 10 restaurants by betweenness centrality:\")\n",
    "for node, score in top_n(betweenness_centrality):\n",
    "    print(f\"{G.nodes[node]['name']}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 10 restaurants by eigenvector centrality:\")\n",
    "for node, score in top_n(eigenvector_centrality):\n",
    "    print(f\"{G.nodes[node]['name']}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0719990",
   "metadata": {},
   "source": [
    "#### 5: Popularity vs Centrality Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6540bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with popularity_generic:\n",
    "\n",
    "# Reorganization of popularity generic, by taking the rank (between -1 and 1, -1 for no ranking, 0 for a good one and 1 for a bad one)\n",
    "\n",
    "ranking = []\n",
    "for i in range(len(G.nodes())):\n",
    "    raw_rank = G.nodes[i][\"popularity_generic\"]\n",
    "    if str(raw_rank) == \"nan\":\n",
    "        ranking.append(-1)\n",
    "    else:\n",
    "        raw_rank = raw_rank.split(\" \")\n",
    "        rank = int(raw_rank[0].replace(\"#\", ' '))\n",
    "        max_rank = int(raw_rank[2])\n",
    "        ranking.append(rank/max_rank)\n",
    "\n",
    "# Plotting correlation between popularity generic and centrality\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
    "\n",
    "axes[0].scatter(pd.Series(degree_centrality), ranking, alpha=0.5)\n",
    "axes[0].set_title(\"Degree vs Popularity\")\n",
    "axes[0].set_xlabel(\"Degree Centrality\")\n",
    "axes[0].set_ylabel(\"Popularity\")\n",
    "axes[0].grid(False)\n",
    "\n",
    "axes[1].scatter(pd.Series(closeness_centrality), ranking, alpha=0.5)\n",
    "axes[1].set_title(\"Closeness vs Popularity\")\n",
    "axes[1].set_xlabel(\"Closeness Centrality\")\n",
    "axes[1].set_ylabel(\"Popularity\")\n",
    "axes[1].grid(False)\n",
    "\n",
    "axes[2].scatter(pd.Series(betweenness_centrality), ranking, alpha=0.5)\n",
    "axes[2].set_title(\"Betweenness vs Popularity\")\n",
    "axes[2].set_xlabel(\"Betweenness Centrality\")\n",
    "axes[2].set_ylabel(\"Popularity\")\n",
    "axes[2].grid(False)\n",
    "\n",
    "axes[3].scatter(pd.Series(eigenvector_centrality), ranking, alpha=0.5)\n",
    "axes[3].set_title(\"Eigenvector vs Popularity\")\n",
    "axes[3].set_xlabel(\"Eigenvector Centrality\")\n",
    "axes[3].set_ylabel(\"Popularity\")\n",
    "axes[3].grid(False)\n",
    "\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with total_reviews_count:\n",
    "\n",
    "ranking = []\n",
    "for i in range(len(G.nodes())):\n",
    "    ranking.append(G.nodes[i][\"total_reviews_count\"])\n",
    "\n",
    "# Plotting correlation between total_reviews_count and centrality\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
    "\n",
    "axes[0].scatter(pd.Series(degree_centrality), ranking, alpha=0.5)\n",
    "axes[0].set_title(\"Degree vs Total Review\")\n",
    "axes[0].set_xlabel(\"Degree Centrality\")\n",
    "axes[0].set_ylabel(\"total_reviews_count\")\n",
    "axes[0].grid(False)\n",
    "\n",
    "axes[1].scatter(pd.Series(closeness_centrality), ranking, alpha=0.5)\n",
    "axes[1].set_title(\"Closeness vs Total Review\")\n",
    "axes[1].set_xlabel(\"Closeness Centrality\")\n",
    "axes[1].set_ylabel(\"total_reviews_count\")\n",
    "axes[1].grid(False)\n",
    "\n",
    "axes[2].scatter(pd.Series(betweenness_centrality), ranking, alpha=0.5)\n",
    "axes[2].set_title(\"Betweenness vs Total Review\")\n",
    "axes[2].set_xlabel(\"Betweenness Centrality\")\n",
    "axes[2].set_ylabel(\"total_reviews_count\")\n",
    "axes[2].grid(False)\n",
    "\n",
    "axes[3].scatter(pd.Series(eigenvector_centrality), ranking, alpha=0.5)\n",
    "axes[3].set_title(\"Eigenvector vs Total Review\")\n",
    "axes[3].set_xlabel(\"Eigenvector Centrality\")\n",
    "axes[3].set_ylabel(\"total_reviews_count\")\n",
    "axes[3].grid(False)\n",
    "\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with popularity generic:\n",
    "\n",
    "ranking = []\n",
    "for i in range(len(G.nodes())):\n",
    "    ranking.append(G.nodes[i][\"avg_rating\"])\n",
    "\n",
    "# Plotting correlation between popularity generic and centrality\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
    "\n",
    "axes[0].scatter(pd.Series(degree_centrality), ranking, alpha=0.5)\n",
    "axes[0].set_title(\"Degree vs Average Rating\")\n",
    "axes[0].set_xlabel(\"Degree Centrality\")\n",
    "axes[0].set_ylabel(\"Average Rating\")\n",
    "axes[0].grid(False)\n",
    "\n",
    "axes[1].scatter(pd.Series(closeness_centrality), ranking, alpha=0.5)\n",
    "axes[1].set_title(\"Closeness vs Average Rating\")\n",
    "axes[1].set_xlabel(\"Closeness Centrality\")\n",
    "axes[1].set_ylabel(\"Average Rating\")\n",
    "axes[1].grid(False)\n",
    "\n",
    "axes[2].scatter(pd.Series(betweenness_centrality), ranking, alpha=0.5)\n",
    "axes[2].set_title(\"Betweenness vs Average Rating\")\n",
    "axes[2].set_xlabel(\"Betweenness Centrality\")\n",
    "axes[2].set_ylabel(\"Average Rating\")\n",
    "axes[2].grid(False)\n",
    "\n",
    "axes[3].scatter(pd.Series(eigenvector_centrality), ranking, alpha=0.5)\n",
    "axes[3].set_title(\"Eigenvector vs Average Rating\")\n",
    "axes[3].set_xlabel(\"Eigenvector Centrality\")\n",
    "axes[3].set_ylabel(\"Average Rating\")\n",
    "axes[3].grid(False)\n",
    "\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c765c3d",
   "metadata": {},
   "source": [
    "#### 6: Community Detection (Unweighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3818fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Louvain on G (non-weighted)\n",
    "G_unweighted = G.copy()\n",
    "for u, v, d in G_unweighted.edges(data=True):\n",
    "    d.pop('weight', None)\n",
    "\n",
    "partition = community_louvain.best_partition(G_unweighted)\n",
    "nx.set_node_attributes(G_unweighted, partition, \"community\")\n",
    "\n",
    "# Print statistics\n",
    "from collections import Counter\n",
    "comm_counter = Counter(partition.values())\n",
    "print(f\"Number of detected communities: {len(comm_counter)}\")\n",
    "print(f\"Length of biggest communities : {comm_counter.most_common(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d36e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ids = [G_unweighted.nodes[n][\"community\"] for n in G_unweighted.nodes()]\n",
    "num_comms = max(community_ids) + 1\n",
    "cmap = plt.cm.get_cmap('nipy_spectral', num_comms)\n",
    "pos = nx.spring_layout(G_unweighted, k=1)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "nodes = nx.draw_networkx_nodes(\n",
    "    G_unweighted,\n",
    "    pos,\n",
    "    node_color=community_ids,\n",
    "    cmap=cmap,\n",
    "    node_size=30\n",
    ")\n",
    "nx.draw_networkx_edges(G_unweighted, pos, alpha=0.05, width=0.2)\n",
    "\n",
    "plt.title(\"Communities colored by Louvain ID\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "cbar = plt.colorbar(nodes, ticks=range(num_comms))\n",
    "cbar.set_label(\"Community ID\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19718db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ids = [G_unweighted.nodes[n][\"community\"] for n in G_unweighted.nodes()]\n",
    "num_comms = max(community_ids) + 1\n",
    "cmap = plt.cm.get_cmap('nipy_spectral', num_comms)\n",
    "pos = nx.spring_layout(G_unweighted, k=1)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "nodes = nx.draw_networkx_nodes(\n",
    "    G_unweighted,\n",
    "    pos,\n",
    "    node_color=community_ids,\n",
    "    cmap=cmap,\n",
    "    node_size=30\n",
    ")\n",
    "#nx.draw_networkx_edges(G_unweighted, pos, alpha=0.05, width=0.2)\n",
    "\n",
    "plt.title(\"Communities colored by Louvain ID\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "cbar = plt.colorbar(nodes, ticks=range(num_comms))\n",
    "cbar.set_label(\"Community ID\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ebbee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_community_features(G_unweighted, community_id, top_n=3):\n",
    "    nodes_in_community = [n for n, d in G_unweighted.nodes(data=True) if d[\"community\"] == community_id]\n",
    "    cuisines = []\n",
    "    diets = []\n",
    "    features = []\n",
    "\n",
    "    for n in nodes_in_community:\n",
    "        cuisines.extend(G_unweighted.nodes[n].get(\"cuisines\", []))\n",
    "        diets.extend(G_unweighted.nodes[n].get(\"special_diets\", []))\n",
    "        features.extend(G_unweighted.nodes[n].get(\"features\", []))\n",
    "\n",
    "    print(f\"\\nCommunity {community_id} - {len(nodes_in_community)} restaurants\")\n",
    "    print(\"Top cuisines:\", Counter(cuisines).most_common(top_n))\n",
    "    print(\"Top diets:\", Counter(diets).most_common(top_n))\n",
    "    print(\"Top features:\", Counter(features).most_common(top_n))\n",
    "\n",
    "top_3 = [comm for comm, _ in comm_counter.most_common(5)]\n",
    "for comm_id in top_3:\n",
    "    summarize_community_features(G_unweighted, comm_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c770d03a",
   "metadata": {},
   "source": [
    "#### 7: Community Detection (Weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22be029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Louvain on G\n",
    "\n",
    "partition = community_louvain.best_partition(G)\n",
    "nx.set_node_attributes(G, partition, \"community\")\n",
    "\n",
    "# Print statistics\n",
    "from collections import Counter\n",
    "comm_counter = Counter(partition.values())\n",
    "print(f\"Number of detected communities: {len(comm_counter)}\")\n",
    "print(f\"Length of biggest communities : {comm_counter.most_common(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ecbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ids = [G.nodes[n][\"community\"] for n in G.nodes()]\n",
    "num_comms = max(community_ids) + 1\n",
    "cmap = plt.cm.get_cmap('nipy_spectral', num_comms)\n",
    "pos = nx.spring_layout(G, k=1)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "nodes = nx.draw_networkx_nodes(\n",
    "    G,\n",
    "    pos,\n",
    "    node_color=community_ids,\n",
    "    cmap=cmap,\n",
    "    node_size=30\n",
    ")\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.05, width=0.2)\n",
    "\n",
    "plt.title(\"Communities colored by Louvain ID\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "cbar = plt.colorbar(nodes, ticks=range(num_comms))\n",
    "cbar.set_label(\"Community ID\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ids = [G.nodes[n][\"community\"] for n in G.nodes()]\n",
    "num_comms = max(community_ids) + 1\n",
    "cmap = plt.cm.get_cmap('nipy_spectral', num_comms)\n",
    "pos = nx.spring_layout(G, k=1)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "nodes = nx.draw_networkx_nodes(\n",
    "    G,\n",
    "    pos,\n",
    "    node_color=community_ids,\n",
    "    cmap=cmap,\n",
    "    node_size=30\n",
    ")\n",
    "# nx.draw_networkx_edges(G, pos, alpha=0.05, width=0.2)\n",
    "\n",
    "plt.title(\"Communities colored by Louvain ID\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "cbar = plt.colorbar(nodes, ticks=range(num_comms))\n",
    "cbar.set_label(\"Community ID\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d4816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_community_features(G, community_id, top_n=3):\n",
    "    nodes_in_community = [n for n, d in G.nodes(data=True) if d[\"community\"] == community_id]\n",
    "    cuisines = []\n",
    "    diets = []\n",
    "    features = []\n",
    "\n",
    "    for n in nodes_in_community:\n",
    "        cuisines.extend(G.nodes[n].get(\"cuisines\", []))\n",
    "        diets.extend(G.nodes[n].get(\"special_diets\", []))\n",
    "        features.extend(G.nodes[n].get(\"features\", []))\n",
    "\n",
    "    print(f\"\\nCommunity {community_id} - {len(nodes_in_community)} restaurants\")\n",
    "    print(\"Top cuisines:\", Counter(cuisines).most_common(top_n))\n",
    "    print(\"Top diets:\", Counter(diets).most_common(top_n))\n",
    "    print(\"Top features:\", Counter(features).most_common(top_n))\n",
    "\n",
    "top = [comm for comm, _ in comm_counter.most_common(5)]\n",
    "for comm_id in top:\n",
    "    summarize_community_features(G, comm_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac345e1f",
   "metadata": {},
   "source": [
    "#### 8 : Role of Dietary Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afabed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering nodes with diet tags\n",
    "\n",
    "diet_tags = {\"Vegetarian Friendly\", \"Vegan Options\", \"Gluten Free Options\"}\n",
    "\n",
    "diet_nodes = [node for node in G.nodes if any(tag in G.nodes[node]['special_diets'] for tag in diet_tags)]\n",
    "\n",
    "non_diet_nodes = list(set(G.nodes()) - set(diet_nodes))\n",
    "\n",
    "# Degrees\n",
    "\n",
    "diet_degrees = [degree_centrality[n] for n in diet_nodes]\n",
    "non_diet_degrees = [degree_centrality[n] for n in non_diet_nodes]\n",
    "\n",
    "# Community inclusion\n",
    "\n",
    "diet_comms = [G.nodes[n]['community'] for n in diet_nodes if 'community' in G.nodes[n]]\n",
    "non_diet_comms = [G.nodes[n]['community'] for n in non_diet_nodes if 'community' in G.nodes[n]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing results\n",
    "\n",
    "avg_degree_diet_degrees = np.average(diet_degrees)\n",
    "avg_degree_non_diet_degrees = np.average(non_diet_degrees)\n",
    "\n",
    "med_degree_diet_degrees = np.median(diet_degrees)\n",
    "med_degree_non_diet_degrees = np.median(non_diet_degrees)\n",
    "\n",
    "avg_comms_diet_comms = np.average(diet_comms)\n",
    "avg_comms_non_diet_comms = np.average(non_diet_comms)\n",
    "\n",
    "med_comms_diet_comms = np.median(diet_comms)\n",
    "med_comms_non_diet_comms = np.median(non_diet_comms)\n",
    "\n",
    "print(f\"Average degree centrality for diet nodes: {avg_degree_diet_degrees}\")\n",
    "print(f\"Average degree centrality for non diet nodes: {avg_degree_non_diet_degrees}\")\n",
    "print(f\"Median degree centrality for diet nodes: {med_degree_diet_degrees}\")\n",
    "print(f\"Mediam degree centrality for non diet nodes: {med_degree_non_diet_degrees}\\n\")\n",
    "\n",
    "print(f\"Average community inclusion for diet nodes: {avg_comms_diet_comms}\")\n",
    "print(f\"Average community inclusion for non diet nodes: {avg_comms_non_diet_comms}\")\n",
    "print(f\"Median community inclusion for diet nodes: {med_comms_diet_comms}\")\n",
    "print(f\"Mediam community inclusion for non diet nodes: {med_comms_non_diet_comms}\\n\")\n",
    "\n",
    "print(f\"Number of diet nodes: {len(diet_nodes)}\")\n",
    "print(f\"Number of non diet nodes: {len(non_diet_nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c147a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subgraphs\n",
    "diet_subgraph = G.subgraph(diet_nodes)\n",
    "non_diet_subgraph = G.subgraph(non_diet_nodes)\n",
    "\n",
    "# Density\n",
    "diet_density = nx.density(diet_subgraph)\n",
    "non_diet_density = nx.density(non_diet_subgraph)\n",
    "\n",
    "# Printing\n",
    "print(f\"Density of diet subgraph: {diet_density:.4f}\")\n",
    "print(f\"Density of non-diet subgraph: {non_diet_density:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ee3bd",
   "metadata": {},
   "source": [
    "#### 9 : Subnetwork Analysis by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2e165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing cities and countries\n",
    "\n",
    "cities = []\n",
    "for element in np.array(df_sample['original_location']):\n",
    "    cities.append(element.replace(\"[\", \"\").replace(\"]\", \"\").replace('\"', \"\").replace(\" \", \"\").split(\",\")[-1])\n",
    "\n",
    "countries = []\n",
    "for element in df_sample['country']:\n",
    "    countries.append(element)\n",
    "\n",
    "# Counting restaurants in every city and country\n",
    "\n",
    "n_most_common = 10 # change for a larger analyse\n",
    "\n",
    "city_counts = Counter(cities)\n",
    "top_cities = city_counts.most_common(n_most_common)\n",
    "\n",
    "country_counts = Counter(countries)\n",
    "top_countries = country_counts.most_common(n_most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_cities)\n",
    "print(top_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa75e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating subgraphs for top cities (city with the most restaurants)\n",
    "\n",
    "city_stats = []\n",
    "\n",
    "for city, _ in top_cities:\n",
    "    city_nodes = [i for i, loc in enumerate(df_sample['original_location']) if city in loc]\n",
    "    subG = G.subgraph(city_nodes)\n",
    "    \n",
    "    if len(subG) < 2:\n",
    "        continue\n",
    "\n",
    "    if nx.is_connected(subG):\n",
    "        diameter = nx.diameter(subG)\n",
    "    else:\n",
    "        largest_cc = max(nx.connected_components(subG), key=len)\n",
    "        diameter = nx.diameter(subG.subgraph(largest_cc))\n",
    "    \n",
    "    stats = {\n",
    "        \"city\": city,\n",
    "        \"n_restaurants\": len(subG),\n",
    "        \"density\": nx.density(subG),\n",
    "        \"diameter\": diameter,\n",
    "        \"clustering\": nx.average_clustering(subG),\n",
    "    }\n",
    "    city_stats.append(stats)\n",
    "stats_df = pd.DataFrame(city_stats)\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e1bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating subgraphs for top countries (country with the most restaurants)\n",
    "\n",
    "country_stats = []\n",
    "\n",
    "for country, _ in top_countries:\n",
    "    country_nodes = [i for i, loc in enumerate(df_sample['country']) if country in loc]\n",
    "    subG = G.subgraph(country_nodes)\n",
    "    \n",
    "    if len(subG) < 2:\n",
    "        continue\n",
    "\n",
    "    if nx.is_connected(subG):\n",
    "        diameter = nx.diameter(subG)\n",
    "    else:\n",
    "        largest_cc = max(nx.connected_components(subG), key=len)\n",
    "        diameter = nx.diameter(subG.subgraph(largest_cc))\n",
    "    \n",
    "    stats = {\n",
    "        \"country\": country,\n",
    "        \"n_restaurants\": len(subG),\n",
    "        \"density\": nx.density(subG),\n",
    "        \"diameter\": diameter,\n",
    "        \"clustering\": nx.average_clustering(subG),\n",
    "    }\n",
    "    country_stats.append(stats)\n",
    "\n",
    "stats_df = pd.DataFrame(country_stats)\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160423fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing global informations\n",
    "print(f\"n_restaurants: {len(G)} ; density {nx.density(G)} ; clustering {nx.average_clustering(G)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c31e8",
   "metadata": {},
   "source": [
    "#### 10 : Visualization of the Global Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f12239",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_colors = [G.nodes[n][\"community\"] for n in G.nodes()]\n",
    "node_sizes = [df_sample.loc[n, \"total_reviews_count\"] if n in df_sample.index else 1 for n in G.nodes]\n",
    "edge_widths  = [(2 - G[u][v].get(\"weight\", 1)) / 5 for u, v in G.edges]\n",
    "\n",
    "# Normalizing sizes\n",
    "sizes = np.array(node_sizes)\n",
    "sizes = 100 + 500 * (sizes - sizes.min()) / (sizes.max() - sizes.min() + 1e-5)\n",
    "pos = nx.spring_layout(G, seed=42, k=1.5)\n",
    "cmap = plt.cm.get_cmap('nipy_spectral', num_comms)\n",
    "\n",
    "plt.figure(figsize=(100, 100))\n",
    "nodes = nx.draw_networkx_nodes(G, pos,\n",
    "                               node_color=node_colors,\n",
    "                               node_size=node_sizes,\n",
    "                               cmap=cmap,\n",
    "                               alpha=0.8)\n",
    "nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.4)\n",
    "\n",
    "plt.title(\"Global Network (color = community / size = reviews / edge width = weight)\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "plt.colorbar(nodes, label=\"Community ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d0ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_colors = [G.nodes[n][\"community\"] for n in G.nodes()]\n",
    "node_sizes = [df_sample.loc[n, \"total_reviews_count\"] if n in df_sample.index else 1 for n in G.nodes]\n",
    "edge_widths  = [(2 - G[u][v].get(\"weight\", 1)) / 5 for u, v in G.edges]\n",
    "\n",
    "# Normalizing sizes\n",
    "sizes = np.array(node_sizes)\n",
    "sizes = 100 + 500 * (sizes - sizes.min()) / (sizes.max() - sizes.min() + 1e-5)\n",
    "pos = nx.spring_layout(G, seed=42, k=1.5)\n",
    "cmap = plt.cm.get_cmap('nipy_spectral', num_comms)\n",
    "\n",
    "plt.figure(figsize=(100, 100))\n",
    "nodes = nx.draw_networkx_nodes(G, pos,\n",
    "                               node_color=node_colors,\n",
    "                               node_size=node_sizes,\n",
    "                               cmap=cmap,\n",
    "                               alpha=0.8)\n",
    "#nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.4)\n",
    "\n",
    "plt.title(\"Global Network  (color = community / size = reviews / without edges)\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "plt.colorbar(nodes, label=\"Community ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100273b8",
   "metadata": {},
   "source": [
    "#### 11 : Keyword Similarity Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe867da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USELESS IF same_data = True\n",
    "\n",
    "# For this task, as the \"keyword\" column is empty for a lot of restaurants, we will create a new subdataset with only restaurants which have a not-empty \"keywords\" column\n",
    "if same_dataset != True:\n",
    "    df_keywords = df[df['keywords'].apply(lambda x: isinstance(x, list) and len(x) > 0)].copy().reset_index(drop=True)\n",
    "    df_keyword_subset = df_keywords.sample(n=number_of_nodes, random_state=random_state).copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a334eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the vector of keywords\n",
    "\n",
    "df_keyword_subset['keywords_str'] = df_keyword_subset['keywords'].apply(lambda x: ' '.join(x))\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df_keyword_subset['keywords_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489f175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine-similarity matrix\n",
    "cos_sim_matrix = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d853736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the threeshold\n",
    "\n",
    "similarity_threshold = 0.5\n",
    "\n",
    "# graph construction\n",
    "G_keywords = nx.Graph()\n",
    "G_keywords.add_nodes_from(df_keyword_subset.index)\n",
    "\n",
    "# Adding edges\n",
    "for i in range(len(df_keyword_subset)):\n",
    "    for j in range(i + 1, len(df_keyword_subset)):\n",
    "        sim = cos_sim_matrix[i, j]\n",
    "        if sim >= similarity_threshold:\n",
    "            G_keywords.add_edge(i, j, weight=sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9ef7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Number of nodes : {G_keywords.number_of_nodes()}\")\n",
    "print(f\"Number of edges : {G_keywords.number_of_edges()}\")\n",
    "print(f\"Density : {nx.density(G_keywords):.4f}\")\n",
    "print(f\"Average clustering coefficient : {nx.average_clustering(G_keywords):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Louvain-communities detection\n",
    "partition = community_louvain.best_partition(G_keywords, weight='weight')\n",
    "nx.set_node_attributes(G_keywords, partition, 'community')\n",
    "\n",
    "# Communities Analysis\n",
    "comm_counter = Counter(partition.values())\n",
    "print(f\"Number of communities (desc similarity): {len(comm_counter)}\")\n",
    "print(f\"Top 5 largest communities: {comm_counter.most_common(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae04c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ids = [G_keywords.nodes[n][\"community\"] for n in G_keywords.nodes()]\n",
    "num_comms = max(community_ids) + 1\n",
    "cmap = plt.cm.get_cmap('nipy_spectral', num_comms)\n",
    "pos = nx.spring_layout(G_keywords, k = 0.5)\n",
    "\n",
    "plt.figure(figsize=(24, 20))\n",
    "nodes = nx.draw_networkx_nodes(\n",
    "    G_keywords,\n",
    "    pos,\n",
    "    node_color=community_ids,\n",
    "    cmap=cmap,\n",
    "    node_size=30\n",
    ")\n",
    "\n",
    "nx.draw_networkx_edges(G_keywords, pos, alpha=0.1, width=0.5)\n",
    "\n",
    "plt.title(\"Communities colored by Louvain ID\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "cbar = plt.colorbar(nodes, ticks=range(num_comms))\n",
    "cbar.set_label(\"Community ID\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose number of top community to print and number of keywords\n",
    "\n",
    "nb_keywords = 5\n",
    "nb_comm = 9\n",
    "\n",
    "# define the function\n",
    "\n",
    "def summarize_community_keywords(G, community_id, top_n=nb_keywords):\n",
    "    # Summarize top [top_n] community keywords for the community [community_id] defined for G and print it\n",
    "    nodes_in_community = [n for n, d in G.nodes(data=True) if d[\"community\"] == community_id]\n",
    "    keywords = []\n",
    "\n",
    "    for n in nodes_in_community:\n",
    "        k = G.nodes[n].get(\"keywords\")\n",
    "        if k:\n",
    "            keywords.extend(k)\n",
    "\n",
    "    print(f\"\\nCommunity {community_id} - {len(nodes_in_community)} restaurants\")\n",
    "    print(\"Top keywords:\", Counter(keywords).most_common(top_n))\n",
    "\n",
    "# Adding keywords information to the network\n",
    "\n",
    "for idx in G_keywords.nodes:\n",
    "    G_keywords.nodes[idx][\"keywords\"] = df_keyword_subset.iloc[idx][\"keywords\"]\n",
    "\n",
    "top = [comm for comm, _ in comm_counter.most_common(nb_comm)]\n",
    "for comm_id in top:\n",
    "    summarize_community_keywords(G_keywords, comm_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5fc2ab",
   "metadata": {},
   "source": [
    "#### 12 : Backbone Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disparity_filtering(G, alpha_threshold=0.5):\n",
    "    backbone = nx.Graph()\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G[node])\n",
    "        k = len(neighbors)\n",
    "        if k <= 1:\n",
    "            continue\n",
    "        strength = 0\n",
    "        for nbr in neighbors:\n",
    "            weight = G[node][nbr].get('weight', 1.0)\n",
    "            strength += weight\n",
    "        \n",
    "        for nbr in neighbors:\n",
    "            w = G[node][nbr].get('weight', 1.0)\n",
    "            p_ij = w / strength\n",
    "            alpha = 1 - (1 - p_ij) ** (k - 1)\n",
    "\n",
    "            if alpha < alpha_threshold:\n",
    "                backbone.add_edge(node, nbr, weight=w)\n",
    "\n",
    "    return backbone\n",
    "\n",
    "\n",
    "\n",
    "# Application of the filter on G (weighted)\n",
    "\n",
    "backbone_graph = disparity_filtering(G, alpha_threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5aaca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing results\n",
    "print(f\"Backbone size: {backbone_graph.number_of_nodes()} nodes, {backbone_graph.number_of_edges()} edges\")\n",
    "print(f\"Density: {nx.density(backbone_graph):.4f}\")\n",
    "print(f\"Average clustering coefficient: {nx.average_clustering(backbone_graph, weight='weight'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c33c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Louvain-communities detection\n",
    "partition = community_louvain.best_partition(backbone_graph, weight='weight')\n",
    "nx.set_node_attributes(backbone_graph, partition, 'community')\n",
    "\n",
    "# Communities Analysis\n",
    "comm_counter = Counter(partition.values())\n",
    "print(f\"Number of communities (desc similarity): {len(comm_counter)}\")\n",
    "print(f\"Top 5 largest communities: {comm_counter.most_common(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdea022",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ids = [backbone_graph.nodes[n][\"community\"] for n in backbone_graph.nodes()]\n",
    "num_comms = max(community_ids) + 1\n",
    "cmap = plt.cm.get_cmap('nipy_spectral', num_comms)\n",
    "pos = nx.spring_layout(backbone_graph, k = 0.5)\n",
    "\n",
    "plt.figure(figsize=(24, 20))\n",
    "nodes = nx.draw_networkx_nodes(\n",
    "    backbone_graph,\n",
    "    pos,\n",
    "    node_color=community_ids,\n",
    "    cmap=cmap,\n",
    "    node_size=30\n",
    ")\n",
    "\n",
    "#nx.draw_networkx_edges(backbone_graph, pos, alpha=0.1, width=0.5)\n",
    "\n",
    "plt.title(\"Communities colored by Louvain ID\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "cbar = plt.colorbar(nodes, ticks=range(num_comms))\n",
    "cbar.set_label(\"Community ID\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01f883",
   "metadata": {},
   "source": [
    "#### 13 : Recommendation via Network Proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b7c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose distance for recommanded restaurants\n",
    "\n",
    "restaurant_id = 60 # choose restaurant by id of node, put \"random\" if non restaurant choosed\n",
    "nb_recommandations = 10\n",
    "\n",
    "if restaurant_id == \"random\":\n",
    "    target_node = random.choice(list(G.nodes))\n",
    "else:\n",
    "    target_node = restaurant_id\n",
    "\n",
    "print(f\"Choosen restaurant is {target_node}, {G.nodes[target_node][\"name\"]} in {G.nodes[target_node][\"city\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb4d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting path weights\n",
    "\n",
    "path_weight = {}\n",
    "for node in tqdm(G.nodes, total=len(G.nodes), desc=\"Looking to shortest path\"):\n",
    "    if node == target_node:\n",
    "        continue\n",
    "    if G.has_edge(target_node, node):\n",
    "        weight = G[target_node][node]['weight']\n",
    "    else:\n",
    "        try:\n",
    "            weight  = nx.shortest_path_length(G, source=target_node, target=node, weight='weight')\n",
    "        except nx.NetworkXNoPath:\n",
    "            continue\n",
    "    path_weight[node] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best candidates\n",
    "top_k = min(nb_recommandations, len(path_weight))\n",
    "top_recommendations = sorted(path_weight.items(), key=lambda x: x[1], reverse=False)[:top_k]\n",
    "recommended_nodes = [node for node, _ in top_recommendations]\n",
    "\n",
    "# Comparing shared cuisines\n",
    "def safe_set(row):\n",
    "    return set(row) if isinstance(row, list) else set()\n",
    "\n",
    "target_cuisines = safe_set(df_sample.loc[target_node, 'cuisines'])\n",
    "target_features = safe_set(df_sample.loc[target_node, 'features'])\n",
    "target_special_diets = safe_set(df_sample.loc[target_node, 'special_diets'])\n",
    "\n",
    "shared_cuisines = []\n",
    "shared_features = []\n",
    "shared_special_diets = []\n",
    "\n",
    "for node in recommended_nodes:\n",
    "    node_cuisines = safe_set(df_sample.loc[node, 'cuisines'])\n",
    "    node_features = safe_set(df_sample.loc[node, 'features'])\n",
    "    node_special_diets = safe_set(df_sample.loc[node, 'special_diets'])\n",
    "\n",
    "    cuisines = target_cuisines.intersection(node_cuisines)\n",
    "    features = target_features.intersection(node_features)\n",
    "    special_diets = target_special_diets.intersection(node_special_diets)\n",
    "\n",
    "    ratio_cuisines, ratio_features, ratio_special_diets = 0, 0, 0\n",
    "\n",
    "    if len(target_cuisines) > 0:\n",
    "        ratio_cuisines = len(cuisines) / len(target_cuisines)\n",
    "    if len(target_features) > 0:\n",
    "        ratio_features = len(features) / len(target_features)\n",
    "    if len(target_special_diets) > 0:\n",
    "        ratio_special_diets = len(special_diets) / len(target_special_diets)\n",
    "\n",
    "    shared_cuisines.append(ratio_cuisines)\n",
    "    shared_features.append(ratio_features)\n",
    "    shared_special_diets.append(ratio_special_diets)\n",
    "\n",
    "def get_score(top_recommendations):\n",
    "    \"\"\"Calculate the score for every recommendations based on total weight of the path :\n",
    "        - Best score is 10, for a total weight of 0 (impossible to get)\n",
    "        - Worst score is 0 for a total weight of 10 or more (minimum 5 edges in the path to get 10 because weight is between 0 and 2).\"\"\"\n",
    "    scores = np.zeros(len(top_recommendations))\n",
    "    for i in range(len(top_recommendations)):\n",
    "        score = top_recommendations[i][1]\n",
    "        if score > 10:\n",
    "            scores[i] = 0\n",
    "        else:\n",
    "            scores[i] = 10 - top_recommendations[i][1]\n",
    "    return scores\n",
    "\n",
    "scores = get_score(top_recommendations)\n",
    "\n",
    "# Print results\n",
    "\n",
    "print(f\"Recommandation for restaurant {target_node}, {G.nodes[target_node]['name']} in {G.nodes[target_node]['city']} :\")\n",
    "\n",
    "if top_k == 0:\n",
    "    print(\"No recommendations for this restaurant.\")\n",
    "else:\n",
    "    if top_k != nb_recommandations:\n",
    "        print(f\"Only {len(path_weight)} restaurants recommanded\")\n",
    "    print(\"Recommendation scores:\")\n",
    "    for i in range(len(top_recommendations)):\n",
    "        node, _ = top_recommendations[i]\n",
    "        score = scores[i]\n",
    "        print(f\"Restaurant {node}, {G.nodes[node]['name']} in {G.nodes[node]['city']}: {Fore.RED}Score={score:.4f}{Style.RESET_ALL}, {Fore.YELLOW}Shared cuisine ratio={shared_cuisines.pop(0):.2f}{Style.RESET_ALL}, {Fore.GREEN}Shared features ratio={shared_features.pop(0):.2f}{Style.RESET_ALL}, {Fore.CYAN}Shared special diets ratio={shared_special_diets.pop(0):.2f}{Style.RESET_ALL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a068fb",
   "metadata": {},
   "source": [
    "#### 14: Semantic Similarity Analysis Using Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1662f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF and similarity cosinus\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df_keyword_subset['keywords_str'])\n",
    "cos_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Network construction\n",
    "similarity_threshold = 0.4\n",
    "G_semantic = nx.Graph()\n",
    "\n",
    "# Adding nodes\n",
    "for idx in range(len(df_keyword_subset)):\n",
    "    G_semantic.add_node(idx, name=df_keyword_subset.iloc[idx]['restaurant_link'])\n",
    "\n",
    "# Adding edges\n",
    "for i in range(len(df_keyword_subset)):\n",
    "    for j in range(i + 1, len(df_keyword_subset)):\n",
    "        sim = cos_sim_matrix[i, j]\n",
    "        if sim > similarity_threshold:\n",
    "            G_semantic.add_edge(i, j, weight=sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f500fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze\n",
    "print(f\"Number of nodes : {G_semantic.number_of_nodes()}\")\n",
    "print(f\"Number of edges : {G_semantic.number_of_edges()}\")\n",
    "print(f\"Density : {nx.density(G_semantic):.4f}\")\n",
    "print(f\"Average clustering coefficient : {nx.average_clustering(G_semantic):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea73b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community detection\n",
    "\n",
    "partition = community_louvain.best_partition(G_semantic)\n",
    "nx.set_node_attributes(G_semantic, partition, \"community\")\n",
    "\n",
    "comm_counter = Counter(partition.values())\n",
    "print(f\"Number of communities (desc similarity): {len(comm_counter)}\")\n",
    "print(f\"Top 5 largest communities: {comm_counter.most_common(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78309e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualisation\n",
    "\n",
    "community_ids = [G_semantic.nodes[n][\"community\"] for n in G_semantic.nodes()]\n",
    "num_comms = max(community_ids) + 1\n",
    "cmap = plt.cm.get_cmap('nipy_spectral', num_comms)\n",
    "pos = nx.spring_layout(G_semantic)\n",
    "\n",
    "plt.figure(figsize=(24, 20))\n",
    "nodes = nx.draw_networkx_nodes(\n",
    "    G_semantic,\n",
    "    pos,\n",
    "    node_color=community_ids,\n",
    "    cmap=cmap,\n",
    "    node_size=30\n",
    ")\n",
    "\n",
    "nx.draw_networkx_edges(G_semantic, pos, alpha=0.1, width=1)\n",
    "\n",
    "plt.title(\"Communities colored by Louvain ID\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "cbar = plt.colorbar(nodes, ticks=range(num_comms))\n",
    "cbar.set_label(\"Community ID\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa16d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose number of top community to print and number of keywords\n",
    "\n",
    "nb_keywords = 5\n",
    "nb_comm = 5\n",
    "\n",
    "# define the function\n",
    "\n",
    "def summarize_community_keywords(G, community_id, top_n=nb_keywords):\n",
    "    # Summarize top [top_n] community keywords for the community [community_id] defined for G and print it\n",
    "    nodes_in_community = [n for n, d in G.nodes(data=True) if d[\"community\"] == community_id]\n",
    "    keywords = []\n",
    "\n",
    "    for n in nodes_in_community:\n",
    "        k = G.nodes[n].get(\"keywords\")\n",
    "        if k:\n",
    "            keywords.extend(k)\n",
    "\n",
    "    print(f\"\\nCommunity {community_id} - {len(nodes_in_community)} restaurants\")\n",
    "    print(\"Top keywords:\", Counter(keywords).most_common(top_n))\n",
    "\n",
    "# Adding keywords information to the network\n",
    "\n",
    "for idx in G_semantic.nodes:\n",
    "    G_semantic.nodes[idx][\"keywords\"] = df_keyword_subset.iloc[idx][\"keywords\"]\n",
    "\n",
    "top = [comm for comm, _ in comm_counter.most_common(nb_comm)]\n",
    "for comm_id in top:\n",
    "    summarize_community_keywords(G_semantic, comm_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59314c40",
   "metadata": {},
   "source": [
    "#### 15. Multi-Attribute Role Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds\n",
    "high_rating_thresh = 4.5\n",
    "high_reviews_thresh = df['total_reviews_count'].quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bcdc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify role\n",
    "def classify_role(row):\n",
    "    rating = row['avg_rating']\n",
    "    awards = bool(row['awards']) if not pd.isnull(row['awards']) else False\n",
    "    reviews = row['total_reviews_count']\n",
    "    veg = row.get('vegetarian_friendly', False)\n",
    "    vegan = row.get('vegan_options', False)\n",
    "    gluten = row.get('gluten_free', False)\n",
    "    dietary = veg and vegan and gluten\n",
    "\n",
    "    if rating >= high_rating_thresh and awards and reviews >= high_reviews_thresh:\n",
    "        return 'premium'\n",
    "    elif dietary and not (awards or rating >= high_rating_thresh):\n",
    "        return 'specialist'\n",
    "    elif reviews >= high_reviews_thresh:\n",
    "        return 'popular hub'\n",
    "    else:\n",
    "        return 'standard'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a0b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply classification\n",
    "df_sample['role'] = df_sample.apply(classify_role, axis=1)\n",
    "\n",
    "# Assign role to each node\n",
    "nx.set_node_attributes(G, df_sample['role'].to_dict(), 'role')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b73cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization with color mapping by role\n",
    "role_colors = {\n",
    "    'premium': 'gold',\n",
    "    'specialist': 'green',\n",
    "    'popular hub': 'blue',\n",
    "    'standard': 'gray'\n",
    "}\n",
    "\n",
    "colors = [role_colors.get(G.nodes[n].get('role', 'standard')) for n in G.nodes]\n",
    "node_sizes = [df_sample.loc[n, \"total_reviews_count\"] if n in df_sample.index else 1 for n in G.nodes]\n",
    "edge_widths  = [(2 - G[u][v].get(\"weight\", 1)) / 5 for u, v in G.edges]\n",
    "pos = nx.spring_layout(G, seed=42, k=1)\n",
    "\n",
    "plt.figure(figsize=(100, 100))\n",
    "nx.draw_networkx_nodes(G, pos, node_color=colors, node_size=node_sizes, cmap=cmap, alpha=0.8)\n",
    "#nx.draw_networkx_edges(G, pos, width=edge_widths , alpha=0.4) #better to don't plot edges for visibility\n",
    "patches = [mpatches.Patch(color=color, label=role) for role, color in role_colors.items()]\n",
    "plt.legend(handles=patches, loc='upper right')\n",
    "plt.title(\"Network Visualization by Restaurant Role with (color = role / size = reviews / edge width = weight)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128e85f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization with color mapping by role\n",
    "role_colors = {\n",
    "    'premium': 'gold',\n",
    "    'specialist': 'green',\n",
    "    'popular hub': 'blue',\n",
    "    'standard': 'gray'\n",
    "}\n",
    "\n",
    "colors = [role_colors.get(G.nodes[n].get('role', 'standard')) for n in G.nodes]\n",
    "pos = nx.spring_layout(G, seed=42, k=0.8)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "nx.draw_networkx_nodes(G, pos, node_color=colors, node_size=30, cmap=cmap, alpha=0.8)\n",
    "nx.draw_networkx_edges(G, pos, width=0.1, alpha=0.4) #better to don't plot edges for visibility\n",
    "patches = [mpatches.Patch(color=color, label=role) for role, color in role_colors.items()]\n",
    "plt.legend(handles=patches, loc='upper right')\n",
    "plt.title(\"Network Visualization by Restaurant Role with color = role\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf5613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality analysis per role\n",
    "centrality = nx.degree_centrality(G)\n",
    "role_centrality = {role: [] for role in role_colors}\n",
    "for node, cent in centrality.items():\n",
    "    role = G.nodes[node].get('role')\n",
    "    if role:\n",
    "        role_centrality[role].append(cent)\n",
    "\n",
    "# Print average centrality by role\n",
    "print(\"\\nAverage centrality by role:\")\n",
    "for role, values in role_centrality.items():\n",
    "    print(f\"{role}: {np.mean(values):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd619c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree analysis per role\n",
    "degree = nx.degree(G)\n",
    "role_degree = {role: [] for role in role_colors}\n",
    "for node, cent in degree:\n",
    "    role = G.nodes[node].get('role')\n",
    "    if role:\n",
    "        role_degree[role].append(cent)\n",
    "\n",
    "# Print average Degree by role\n",
    "print(\"\\nAverage degree by role:\")\n",
    "for role, values in role_degree.items():\n",
    "    print(f\"{role}: {np.mean(values):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ec15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_counts = df_sample['role'].value_counts()\n",
    "\n",
    "print(\"Number of restaurants by role :\")\n",
    "print(role_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5fa61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_communities = {role: [] for role in role_colors}\n",
    "for node in G.nodes:\n",
    "    role = G.nodes[node].get('role')\n",
    "    community = G.nodes[node].get('community')\n",
    "    if role in role_communities and community is not None:\n",
    "        role_communities[role].append(community)\n",
    "\n",
    "print(\"\\nAverage community inclusion by role:\")\n",
    "for role, communities in role_communities.items():\n",
    "    avg_comm = np.mean(communities) if communities else 0\n",
    "    print(f\"{role}: {avg_comm:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
